{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/widdows/.local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, DataCollatorWithPadding, Trainer, TrainingArguments\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from eval_tool import sample_from_big_string, sample_texts_from_dir\n",
    "\n",
    "\n",
    "HUGGINGFACE_MODEL = \"distilbert/distilbert-base-multilingual-cased\"\n",
    "OUTPUT_DIR = \"distilmbert_lc_model_80_b\"\n",
    "num_train_per_lang = 80\n",
    "train_len = 256\n",
    "num_test_per_lang = 20\n",
    "test_len = 256\n",
    "\n",
    "\n",
    "TOKENIZER = AutoTokenizer.from_pretrained(HUGGINGFACE_MODEL)\n",
    "def preprocess_function(examples):\n",
    "    return TOKENIZER(examples[\"text\"], max_length=512, truncation=True)\n",
    "\n",
    "\n",
    "ACCURACY = evaluate.load(\"accuracy\")\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return ACCURACY.compute(predictions=predictions, references=labels)\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    precision, recall, fscore, _ = precision_recall_fscore_support(labels, predictions, average='weighted')\n",
    "    return {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'fscore': fscore\n",
    "    }\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=TOKENIZER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:9585 training characters for language 'bug'. We need 20480.\n",
      "WARNING:root:2394 training characters for language 'bug'. We need 5120.\n",
      "WARNING:root:Skipping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:10895 training characters for language 'iu'. We need 20480.\n",
      "WARNING:root:2720 training characters for language 'iu'. We need 5120.\n",
      "WARNING:root:Skipping\n",
      "WARNING:root:2256 training characters for language 'chy'. We need 20480.\n",
      "WARNING:root:557 training characters for language 'chy'. We need 5120.\n",
      "WARNING:root:Skipping\n",
      "WARNING:root:12836 training characters for language 'bi'. We need 20480.\n",
      "WARNING:root:3206 training characters for language 'bi'. We need 5120.\n",
      "WARNING:root:Skipping\n",
      "WARNING:root:13542 training characters for language 'ty'. We need 20480.\n",
      "WARNING:root:3386 training characters for language 'ty'. We need 5120.\n",
      "WARNING:root:Skipping\n",
      "WARNING:root:5236 training characters for language 'ik'. We need 20480.\n",
      "WARNING:root:1301 training characters for language 'ik'. We need 5120.\n",
      "WARNING:root:Skipping\n",
      "WARNING:root:13646 training characters for language 'sg'. We need 20480.\n",
      "WARNING:root:3409 training characters for language 'sg'. We need 5120.\n",
      "WARNING:root:Skipping\n",
      "WARNING:root:2507 training characters for language 'cr'. We need 20480.\n",
      "WARNING:root:621 training characters for language 'cr'. We need 5120.\n",
      "WARNING:root:Skipping\n",
      "Map: 100%|██████████| 22560/22560 [00:03<00:00, 7501.15 examples/s]\n",
      "Map: 100%|██████████| 5640/5640 [00:00<00:00, 8118.87 examples/s]\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Small Wikipedia corpus from https://lukelindemann.com/wiki_corpus.html, preprocessed using lplangid training script.\n",
    "wiki_root = Path.home() / \"Data\" / \"WikipediaLindemann\"\n",
    "language_codes = os.listdir(wiki_root / \"train\")\n",
    "label2id = {lang: idx for idx, lang in enumerate(language_codes)}\n",
    "id2label = {idx: lang for lang, idx in label2id.items()}\n",
    "\n",
    "train_texts, train_labels, test_texts, test_labels = [], [], [], []\n",
    "\n",
    "for lang in language_codes:\n",
    "    train_fh = open(wiki_root / \"train\" / lang, encoding='utf-8')\n",
    "    test_fh = open(wiki_root / \"test\" / lang, encoding='utf-8')\n",
    "    train_contents = train_fh.read()\n",
    "    test_contents = test_fh.read()\n",
    "\n",
    "    # Check there is enough data, otherwise skip.\n",
    "    if len(train_contents) < train_len * num_train_per_lang or len(test_contents) < test_len * num_test_per_lang:\n",
    "        logging.warning(f\"{len(train_contents)} training characters for language '{lang}'. We need {train_len * num_train_per_lang}.\")\n",
    "        logging.warning(f\"{len(test_contents)} training characters for language '{lang}'. We need {test_len * num_test_per_lang}.\")\n",
    "        logging.warning(\"Skipping\")\n",
    "        continue\n",
    "\n",
    "    train_texts.extend(sample_from_big_string(train_contents, train_len, num_train_per_lang))\n",
    "    train_labels.extend([label2id[lang]] * num_train_per_lang)\n",
    "    test_texts.extend(sample_from_big_string(train_contents, test_len, num_test_per_lang))\n",
    "    test_labels.extend([label2id[lang]] * num_test_per_lang)\n",
    "\n",
    "train_dataset = Dataset.from_dict({\"text\": train_texts, \"label\": train_labels})\n",
    "test_dataset = Dataset.from_dict({\"text\": test_texts, \"label\": test_labels})\n",
    "\n",
    "# Bundle into a DatasetDict\n",
    "dataset_dict = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"test\": test_dataset\n",
    "})\n",
    "tokenized_data = dataset_dict.map(preprocess_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        HUGGINGFACE_MODEL,\n",
    "        num_labels=len(label2id), id2label=id2label, label2id=label2id\n",
    "    )\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_data[\"train\"],\n",
    "    eval_dataset=tokenized_data[\"test\"],\n",
    "    tokenizer=TOKENIZER,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training based on distilbert/distilbert-base-multilingual-cased ... outputting to distilbert_lc_model_80_b\n",
      "Labels: 282 Num train: 80 (len 256). Num test: 20 (len 256).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='14100' max='14100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [14100/14100 1:03:18, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Fscore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.835600</td>\n",
       "      <td>1.051048</td>\n",
       "      <td>0.856825</td>\n",
       "      <td>0.860284</td>\n",
       "      <td>0.842108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.523800</td>\n",
       "      <td>0.293300</td>\n",
       "      <td>0.915068</td>\n",
       "      <td>0.920035</td>\n",
       "      <td>0.908701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.258300</td>\n",
       "      <td>0.167055</td>\n",
       "      <td>0.939126</td>\n",
       "      <td>0.945745</td>\n",
       "      <td>0.938646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.172800</td>\n",
       "      <td>0.144305</td>\n",
       "      <td>0.953232</td>\n",
       "      <td>0.945745</td>\n",
       "      <td>0.939434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.137000</td>\n",
       "      <td>0.106313</td>\n",
       "      <td>0.958431</td>\n",
       "      <td>0.957979</td>\n",
       "      <td>0.953299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.119500</td>\n",
       "      <td>0.096519</td>\n",
       "      <td>0.962320</td>\n",
       "      <td>0.962234</td>\n",
       "      <td>0.958400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.089300</td>\n",
       "      <td>0.080763</td>\n",
       "      <td>0.967046</td>\n",
       "      <td>0.968440</td>\n",
       "      <td>0.964432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.089200</td>\n",
       "      <td>0.072717</td>\n",
       "      <td>0.974260</td>\n",
       "      <td>0.970745</td>\n",
       "      <td>0.967565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.082300</td>\n",
       "      <td>0.069296</td>\n",
       "      <td>0.974026</td>\n",
       "      <td>0.971986</td>\n",
       "      <td>0.969491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.080200</td>\n",
       "      <td>0.067643</td>\n",
       "      <td>0.969185</td>\n",
       "      <td>0.973050</td>\n",
       "      <td>0.969544</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done training.\n"
     ]
    }
   ],
   "source": [
    "retrain = True\n",
    "if retrain:\n",
    "    print(f\"Starting training based on {HUGGINGFACE_MODEL} ... outputting to {trainer.args.output_dir}\")\n",
    "    print(f\"Labels: {len(set(test_labels))} Num train: {num_train_per_lang} (len {train_len}). Num test: {num_test_per_lang} (len {test_len}).\")\n",
    "    trainer.train()\n",
    "    print(\"Done training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='705' max='705' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [705/705 00:19]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval_loss: 0.0676, eval_precision: 0.9692, eval_recall: 0.9730, eval_fscore: 0.9695, eval_runtime: 20.5156, eval_samples_per_second: 274.9120, eval_steps_per_second: 34.3640\n"
     ]
    }
   ],
   "source": [
    "lc_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    OUTPUT_DIR + \"/checkpoint-14100\",\n",
    "    num_labels=len(label2id), id2label=id2label, label2id=label2id)\n",
    "\n",
    "evaluator = Trainer(\n",
    "    model=lc_model,\n",
    "    eval_dataset=tokenized_data[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "test_results = evaluator.evaluate(tokenized_data[\"test\"])\n",
    "print(\", \".join([f\"{k}: {v:0.4f}\" for k, v in test_results.items()]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 5800/5800 [00:00<00:00, 48306.12 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='725' max='725' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [725/725 00:04]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 5800/5800 [00:00<00:00, 16108.81 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='725' max='725' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [725/725 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 5800/5800 [00:00<00:00, 23825.13 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='725' max='725' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [725/725 00:06]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 5800/5800 [00:00<00:00, 14059.37 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='725' max='725' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [725/725 00:11]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 5800/5800 [00:00<00:00, 8162.97 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='725' max='725' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [725/725 00:19]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for length 16\n",
      "eval_loss: 2.9565, eval_precision: 0.5560, eval_recall: 0.4667, eval_fscore: 0.4619, eval_runtime: 4.8304, eval_samples_per_second: 1200.7290, eval_steps_per_second: 150.0910\n",
      "Results for length 32\n",
      "eval_loss: 2.0615, eval_precision: 0.6743, eval_recall: 0.6347, eval_fscore: 0.6260, eval_runtime: 5.0815, eval_samples_per_second: 1141.3880, eval_steps_per_second: 142.6730\n",
      "Results for length 64\n",
      "eval_loss: 1.4544, eval_precision: 0.7739, eval_recall: 0.7717, eval_fscore: 0.7599, eval_runtime: 6.7701, eval_samples_per_second: 856.7070, eval_steps_per_second: 107.0880\n",
      "Results for length 128\n",
      "eval_loss: 1.1765, eval_precision: 0.8394, eval_recall: 0.8419, eval_fscore: 0.8306, eval_runtime: 11.0415, eval_samples_per_second: 525.2910, eval_steps_per_second: 65.6610\n",
      "Results for length 256\n",
      "eval_loss: 0.8697, eval_precision: 0.8738, eval_recall: 0.8803, eval_fscore: 0.8669, eval_runtime: 19.7455, eval_samples_per_second: 293.7380, eval_steps_per_second: 36.7170\n"
     ]
    }
   ],
   "source": [
    "all_results = {}\n",
    "for eval_strlen in [16, 32, 64, 128, 256]:\n",
    "    eval_texts, eval_labels = sample_texts_from_dir(Path(wiki_root) / \"test\", eval_strlen, num_test_per_lang)\n",
    "\n",
    "    eval_dataset = Dataset.from_dict({\"text\": eval_texts, \"label\": [label2id[l] for l in eval_labels]})\n",
    "    tokenized_eval_data = eval_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "    evaluator = Trainer(\n",
    "        model=lc_model,\n",
    "        eval_dataset=tokenized_eval_data,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    eval_results = evaluator.evaluate(tokenized_eval_data)\n",
    "    all_results[eval_strlen] = eval_results\n",
    "\n",
    "\n",
    "for eval_strlen, eval_results in all_results.items():\n",
    "    print(f\"Results for length {eval_strlen}\")\n",
    "    print(\", \".join([f\"{k}: {v:0.4f}\" for k, v in eval_results.items()]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
